<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yyttm.github.io/</id>
    <title>相由心生</title>
    <updated>2024-04-24T14:49:09.245Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yyttm.github.io/"/>
    <link rel="self" href="https://yyttm.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://yyttm.github.io/images/avatar.png</logo>
    <icon>https://yyttm.github.io/favicon.ico</icon>
    <rights>All rights reserved 2024, 相由心生</rights>
    <entry>
        <title type="html"><![CDATA[Hadoop学习案例——实现主播的播放量等数据的统计以及TopN排序]]></title>
        <id>https://yyttm.github.io/post/hadoop-xue-xi-an-li-shi-xian-zhu-bo-de-bo-fang-liang-deng-shu-ju-de-tong-ji-yi-ji-topn-pai-xu/</id>
        <link href="https://yyttm.github.io/post/hadoop-xue-xi-an-li-shi-xian-zhu-bo-de-bo-fang-liang-deng-shu-ju-de-tong-ji-yi-ji-topn-pai-xu/">
        </link>
        <updated>2024-04-24T05:56:40.000Z</updated>
        <content type="html"><![CDATA[<!--more-->
<h2 id="项目流程">项目流程</h2>
<blockquote>
<p>数据集准备：video_rating.log<br>
链接:<a href="https://yyttm.lanzoub.com/iH7tb1wbm0xa">video_rating.log</a></p>
</blockquote>
<figure data-type="image" tabindex="1"><img src="https://img2.imgtp.com/2024/04/24/ltD7WII1.png" alt="1713938991336.png" loading="lazy"></figure>
<h3 id="创建maven工程项目">创建Maven工程项目</h3>
<ol>
<li>创建Maven<br>
打开idea新建一个项目，选择Maven点击下一步，添加项目名称，路径</li>
<li>添加依赖<br>
编辑pom.xml配置文件，添加如下内容，等待自动导入完成(注意修改Hadoop版本号)</li>
</ol>
<pre><code class="language-java">		&lt;dependencies&gt;
        &lt;!-- Hadoop相关依赖包--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
            &lt;version&gt;2.7.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
            &lt;version&gt;2.7.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;2.7.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.12&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;!-- 打包编译的插件 --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;source&gt;6&lt;/source&gt;
                    &lt;target&gt;6&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
</code></pre>
<p>这样一个基本的Hadoop的Maven工程项目就创建完毕了</p>
<h3 id="编写mapreduce程序">编写MapReduce程序</h3>
<ol>
<li>Map类<br>
map类的作用是对原始数据的清洗操作，解析JSON格式的数据，获取我们所需要的数据在java目录下新建一个DataCleanMap类，继承Mapper类，复写map方法</li>
</ol>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.codehaus.jettison.json.JSONException;
import org.codehaus.jettison.json.JSONObject;

import java.io.IOException;


public class DataCleanMap extends Mapper&lt;LongWritable, Text, Text, Text&gt; {
    @Override
    protected void map(LongWritable k1, Text v1, Context context) throws IOException, InterruptedException {
        try {
            String line = v1.toString();//获取每一行内容
            JSONObject jsonObj = new JSONObject(line); //将字符串转换为JSON格式
            String id = jsonObj.getString(&quot;uid&quot;); //获取主播的id数据

            int gold = jsonObj.getInt(&quot;gold&quot;);
            int watchnumpv = jsonObj.getInt(&quot;watchnumpv&quot;);
            int follower = jsonObj.getInt(&quot;follower&quot;);
            int length = jsonObj.getInt(&quot;length&quot;);
            // 过滤掉异常数据（如播放量&lt;0等这些）
            if (gold &gt;= 0 &amp;&amp; watchnumpv &gt;= 0 &amp;&amp; follower &gt;= 0 &amp;&amp; length &gt;= 0) {
                // 封装数据到Text中，最后写入context上下文中
                Text k2 = new Text();
                k2.set(id);
                Text v2 = new Text();
                v2.set(gold + &quot;\t&quot; + watchnumpv + &quot;\t&quot; + follower + &quot;\t&quot; + length);
                context.write(k2, v2);
            }
        } catch (JSONException e) {
            e.printStackTrace();
        }

    }
}

</code></pre>
<ol start="2">
<li>主类（入口类）<br>
相当于一个程序的主函数，最先开始执行的地方在java目录下新建一个DataCleanJob类，在主函数设置Job作业的参数信息</li>
</ol>
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 

public class DataCleanJob {
    public static void main(String[] args) {
        try {
            // 运行程序指令输入错误，直接退出程序
            if (args.length != 2) {
                System.exit(100);
            }
            Configuration conf = new Configuration(); //job需要的配置参数
            Job job = Job.getInstance(conf); //创建一个job
            job.setJarByClass(DataCleanJob.class);
            FileInputFormat.setInputPaths(job, new Path(args[0]));//指定输入路径（可以是文件，也可以是目录）
            FileOutputFormat.setOutputPath(job, new Path(args[1])); //指定输出路径（只能是指定一个不存在的目录）
            //指定map相关代码
            job.setMapperClass(DataCleanMap.class);
            //指定K2的类型
            job.setMapOutputKeyClass(Text.class);
            //指定v2的类型
            job.setMapOutputValueClass(Text.class);
            //设置reduce的数量，0表示禁用reduce
            job.setNumReduceTasks(0);
            //提交作业job
            job.waitForCompletion(true);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

</code></pre>
<h3 id="编译打包成jar包上传">编译打包成jar包上传</h3>
<p>展开右侧的Mavan,双击clean清理一下，再双击package生成jar包，目录回多了一个target的文件夹，下面就有一个jar包程序</p>
<h3 id="原始数据上传到hdfs">原始数据上传到HDFS</h3>
<p>将video_rating.log/zhuboClean.jar传输到HDFS中</p>
<pre><code>hdfs dfs -mkdir /usr/hadoop/zhubo
hdfs dfs -put /usr/hadoop/zhubo
hdfs dfs -put zhuboClean.jar /usr/hadoop/zhubo
</code></pre>
<h3 id="运行jar包程序">运行jar包程序</h3>
<p>虚拟机中输入</p>
<p>|  zhuboClean.jar     |  jar包名称  |<br>
| DataCleanJob  |  主类（入口类）类名 |<br>
|  /zhubo/data/  |  输入路径（数据所在目录) |<br>
|  /zhubo/resultClean/ | 输出路径（必须不存在的文件夹) |</p>
<pre><code>hadoop jar zhuboClean.jar DataCleanJob /usr/hadoop/zhubo/ /usr/hadoop/zhubo/out/
</code></pre>
<p>运行结束后使用HDFS命令查看结果</p>
<pre><code>hdfs dfs -cat /usr/hadoop/zhubo/out/part-m-00000
#出现下列字样及成功
120010010285    285     2750    1100    1650
120010010969    969     9590    3836    5754
120010010742    742     7320    2928    4392
120010010287    287     2770    1108    1662
120010010933    933     9230    3692    5538
120010010278    278     2680    1072    1608
12001001035     35      250     100     150
120010010354    354     3440    1376    2064
120010010771    771     7610    3044    4566
120010010932    932     9220    3688    5532
120010010108    108     980     392     588
120010010752    752     7420    2968    4452
120010010356    356     3460    1384    2076
120010010456    456     4460    1784    2676
120010010763    763     7530    3012    4518
120010010183    183     1730    692     1038
120010010235    235     2250    900     1350
120010010904    904     8940    3576    5364
120010010201    201     1910    764     1146

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hadoop学习案例——Map Reduce设计 好友推荐功能]]></title>
        <id>https://yyttm.github.io/post/hadoop-xue-xi-an-li-map-reduce-she-ji-hao-you-tui-jian-gong-neng/</id>
        <link href="https://yyttm.github.io/post/hadoop-xue-xi-an-li-map-reduce-she-ji-hao-you-tui-jian-gong-neng/">
        </link>
        <updated>2024-04-23T14:58:12.000Z</updated>
        <summary type="html"><![CDATA[<p>文章内容主要以案例为主</p>
]]></summary>
        <content type="html"><![CDATA[<p>文章内容主要以案例为主</p>
<!--more-->
<h2 id="项目说明">项目说明</h2>
<ul>
<li>互为推荐关系<br>
- 非好友的两个人之间存在相同好友则互为推荐关系<br>
- 朋友圈两个非好友的人，存在共同好友人数越多，越值得推荐<br>
- 存在一个共同好友，值为1；存在多个值累加</li>
</ul>
<h2 id="准备数据">准备数据</h2>
<blockquote>
<p>数据准备：friend.txt<br>
链接:<a href="https://yyttm.lanzoub.com/iEX2i1w9xqvi">friend.txt</a></p>
</blockquote>
<h2 id="编写map-reduce程序">编写Map Reduce程序</h2>
<blockquote>
<p>前置依赖</p>
</blockquote>
<pre><code class="language-java"> &lt;!-- Hadoop Core --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
      &lt;version&gt;2.7.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- Hadoop Client for MapReduce --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;
      &lt;version&gt;2.7.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- Hadoop Client for HDFS --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
      &lt;version&gt;2.7.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- Hadoop Client for YARN (if you are using YARN) --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt;
      &lt;version&gt;2.7.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- Hadoop Client for Common Utilities --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
      &lt;version&gt;2.7.1&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<blockquote>
<p>项目目录</p>
</blockquote>
<figure data-type="image" tabindex="1"><img src="https://img2.imgtp.com/2024/04/23/28gJuBFi.png" alt="1713886070342.png" loading="lazy"></figure>
<ol>
<li>FriendsRecommendMapper</li>
</ol>
<pre><code class="language-java">package org.hadoop.mr;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.util.StringUtils;

import java.io.IOException;

public class FriendsRecommendMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    Text mkey = new Text();
    IntWritable mval = new IntWritable();

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {
        //不能用双引号,要用单引号 将传递过来的值进行分割
        String[] strs = StringUtils.split(value.toString(), ' ');
        // 直接好友的 key为直接好友列表  value为0
        for (int i = 1; i &lt; strs.length; i++) {
            //直接好友关系
            mkey.set(fof(strs[0], strs[i]));
            mval.set(0);
            context.write(mkey, mval);
            //间接好友关系 设置value为1
            for (int j = i + 1; j &lt; strs.length; j++) {
                mkey.set(fof(strs[i], strs[j]));
                mval.set(1);
                context.write(mkey, mval);
            }
        }
    }

    //两个共同好友的间接好友之间,可能存在 B C 和C B 的情况,但是比对累加时,计算机不识别,所以需要字典排序
    private static String fof(String str1, String str2) {
        //compareTo比较的 正数说明大
        if (str1.compareTo(str2) &gt; 0) {
            return str2 + &quot;:&quot; + str1;
        }
        return str1 + &quot;:&quot; + str2;
    }
}
</code></pre>
<ol start="2">
<li>FriendsRecommendReduce</li>
</ol>
<pre><code class="language-java">package org.hadoop.mr;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class FriendsRecommendReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {

    private IntWritable mValue = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {

        //
        int flg = 0;
        int sum = 0;
        for (IntWritable value : values) {
            if (value.get() == 0) {// 直接关系
                flg = 1;
            }
            sum += value.get(); // 添加间接权重
        }

        if (flg == 0) {
            mValue.set(sum);
            context.write(key, mValue);
        }
    }
}
</code></pre>
<ol start="3">
<li>FriendsRecommend</li>
</ol>
<pre><code class="language-java">package org.hadoop.mr;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


import java.io.IOException;

public class FriendsRecommend {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        job.setJarByClass(FriendsRecommend.class);

        Path input = new Path(args[0]);
        FileInputFormat.addInputPath(job, input);

        Path output = new Path(args[1]);
        //如果文件存在,,删除文件,方便后续调试代码
        if (output.getFileSystem(conf).exists(output)) {
            output.getFileSystem(conf).delete(output,true);
        }

        FileOutputFormat.setOutputPath(job, output);

        job.setMapperClass(FriendsRecommendMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        job.setReducerClass(FriendsRecommendReduce.class);

        job.waitForCompletion(true);
    }
}
</code></pre>
<h2 id="运行程序">运行程序</h2>
<ol>
<li>上传程序至虚拟机</li>
</ol>
<ul>
<li>先把两个文件上传到虚拟机中<br>
<img src="https://img2.imgtp.com/2024/04/23/4uTmcJ1y.png" alt="1713886006675.png" loading="lazy"></li>
</ul>
<ol start="2">
<li>在HDFS中上传文件</li>
</ol>
<ul>
<li>先在hdfs中创建friend目录</li>
</ul>
<pre><code>hdfs dfs -mkdir /usr/hadoop/friend 
</code></pre>
<ul>
<li>将文件上传到HDFS中</li>
</ul>
<pre><code>hdfs dfs -put friend.txt friend.jar /usr/hadoop/friend
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://img2.imgtp.com/2024/04/23/LyxPyg9l.png" alt="1713886671174.png" loading="lazy"></figure>
<ol start="3">
<li>执行friends.jar程序</li>
</ol>
<pre><code>hadoop jar friends.jar org.hadoop.mr.FriendsRecommend /usr/hadoop/friend /usr/hadoop/friend/out
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://img2.imgtp.com/2024/04/23/yjIlKxji.png" alt="1713887026178.png" loading="lazy"></figure>
<ol start="4">
<li>查看结果</li>
</ol>
<pre><code>hdfs dfs -cat /usr/hadoop/friend/out/part-r-00000
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://img2.imgtp.com/2024/04/23/ldscPURS.png" alt="1713887952544.png" loading="lazy"></figure>
<!-- more -->
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[win10/11:定时关机方法]]></title>
        <id>https://yyttm.github.io/post/win1011ding-shi-guan-ji-fang-fa/</id>
        <link href="https://yyttm.github.io/post/win1011ding-shi-guan-ji-fang-fa/">
        </link>
        <updated>2023-09-21T14:35:12.000Z</updated>
        <summary type="html"><![CDATA[<p>这篇文章的灵感是因为女朋友睡觉时需要听歌或者看剧碎觉但是容易睡着导致电脑不能关机，这里给带来定时关机的方法</p>
]]></summary>
        <content type="html"><![CDATA[<p>这篇文章的灵感是因为女朋友睡觉时需要听歌或者看剧碎觉但是容易睡着导致电脑不能关机，这里给带来定时关机的方法</p>
<!-- more -->
<h2 id="原理br-使用关机命令-shutdown">原理<br /> 使用关机命令 shutdown</h2>
<p><strong>win</strong>+<strong>r</strong>输入：</p>
<pre><code>shutdown -s -t 3600
  //时间是以秒为计量单位，3600为1小时
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://yyttm.github.io//post-images/1695308425582.png" alt="" loading="lazy"></figure>
<h3 id="取消定时关机">取消定时关机：</h3>
<pre><code>shutdown - a
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://yyttm.github.io//post-images/1695308518950.png" alt="" loading="lazy"></figure>
<h3 id="查看shutdown用法及参数说明">查看shutdown用法及参数说明</h3>
<p><strong>win+X</strong> 选择*终端(管理员)*输入：</p>
<pre><code>shutdown
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://yyttm.github.io//post-images/1695309013711.png" alt="" loading="lazy"></figure>
<h3 id="指定时间关机">指定时间关机</h3>
<p><strong>win+R</strong> 输入：</p>
<pre><code>at 22:00 shutdown -s
 //设置为晚上22点整关机
</code></pre>
<!-- more -->
<h1 id="小教程">小教程</h1>
<p>如每次都定时为一个时间的话可以新建一个记事本输入：</p>
<pre><code>shutdown -s -t 1800
// 设置为半小时后关机
</code></pre>
<p>保存更改后缀为<strong>bat</strong>下次使用时可以直接点击就可以定时关机，不需要次次输入指令</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[葡萄]]></title>
        <id>https://yyttm.github.io/post/pu-tao/</id>
        <link href="https://yyttm.github.io/post/pu-tao/">
        </link>
        <updated>2023-09-08T12:13:47.000Z</updated>
        <summary type="html"><![CDATA[<p>一篇小记</p>
]]></summary>
        <content type="html"><![CDATA[<p>一篇小记</p>
<!--more-->
<p>今日在抖音上看到有人分享葡萄，青紫青紫的晶莹剔透。有一瞬间感觉直冲心头酸溜溜的,我是不爱吃葡萄的但是家里那棵葡萄树却陪了我很久很久，记得小时候在家里每逢夏天树上会结一串又一串的果子，这葡萄吃进嘴里酸的口水止不住的往下流，我总是不爱吃这些我觉得家里边结的不如外面卖的好葡萄熟的时候奶奶总会隔段时间去剪下一串拿到我的面前问我吃不吃，我也总是不吃一颗并说着不如外面卖的好吃，奶奶看看我也不说什么转身离开了。这葡萄生的并不好看，小的时候和绿豆一样小我路过葡萄树时会掐碎一个，后来葡萄长得花生般大小的时候我就摘一颗放进嘴里，牙齿轻轻一咬汁水流进嘴里我也面露苦涩将其吐了出来，再后来大了些葡萄一个个又圆又青摸起来硬邦邦的这时候我就不再理会了，每逢朋友来的时候就会说这葡萄长得真好，摘一颗放进嘴里不出意外立马吐了出来，我也嘿嘿一笑。小时候的夏天很热，家里总是不让中午出去说夏天外面有拐小孩的，我总是坐在院子里瞅着一颗颗葡萄，想数着有几颗，但有时我也不算听话叫上几个玩伴骑着小车四处溜达。我家附近没有小溪只有几个水库大人们总是叮嘱自己的孩子不要去水里游泳，我胆子小从来不敢，一次在骑车出去玩的路上碰到一个水库玩伴们提出下河游泳，两人很快脱了个精光钻进水里像小白条一样游来游去，近40度的天我在边上替他们拿着衣服，等他们尽性了上来就嘲笑我这么热的天你像傻子一样在这晒着。后来同村的大人看到我们了回家之后就少不了一顿挨打。夏天最开心的事莫不过吹着风扇吃着西瓜，小孩们跑的出一身汗坐在风扇前吃着西瓜，印象里的西瓜总是有股大蒜味，爷爷将葡萄全部摘了下来放进冰箱里，天热的时候去拿上一颗含在嘴里特别解暑，晚上我吵着闹着要和爷爷睡，爷爷特别疼我们几个晚上爷爷将折叠床支在家门口我和爷爷躺在上面，爷爷给我讲故事，从前一家人有三个孩子，一个叫筷子，一个叫勺子，一个叫刷子.....狼外婆找到他们.....我没记得内容就进入了梦乡，夏天最讨厌的就是蚊子，蚊子总是在你意想不到的时候咬你一口，在门外睡总是被叮得一身包，爷爷会将唾沫和一种草混在一块砸碎抹在我身上一会就不痒了，葡萄现在家里还有，但是我仍然不爱吃葡萄陪了我好久好久，爷爷也陪了我好久，现在葡萄还在爷爷却不在了，我讨厌家里的葡萄，我想我爷爷，我怀念我和爷爷在一起的日子，我怀念小时候那个炎热的夏天，我也想家。				<br>
——记于2023.4.21</p>
]]></content>
    </entry>
</feed>